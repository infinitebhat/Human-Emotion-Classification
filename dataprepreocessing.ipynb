{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "dataprepreocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/infinitebhat/Human-Emotion-Classification/blob/main/dataprepreocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAh8MC79588W",
        "outputId": "65c51ebb-ba80-4eb9-98df-e5d0a4331c67"
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Audio\n",
        "import torch as torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio.transforms as audio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import sys\n",
        "from scipy.signal import resample\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/lukasherron/.local/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
            "  warnings.warn(\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFo0xzH5588c"
      },
      "source": [
        "data_training = np.load('data_extra_1.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCqQXjS588d"
      },
      "source": [
        "r = len(data_training)\n",
        "for i in range(r):\n",
        "    data_training[i] = resample(data_training[i], 100000)\n",
        "    data_training[i] = (data_training[i] - np.average(data_training[i]))/max(abs(data_training[i]) + 1e-9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_RyT238588d"
      },
      "source": [
        "spectrograms_1 = []\n",
        "spectrograms_2 = []\n",
        "spectrograms_3 = []\n",
        "for i in range(r):\n",
        "    \n",
        "    sig = torch.Tensor(data_training[i])\n",
        "    sg = audio.MelSpectrogram( \n",
        "                            sample_rate = 44000, n_fft=4096, \n",
        "                            win_length = 4096, hop_length=1024,\n",
        "                            n_mels=128)(sig)\n",
        "    sg = sg.detach().numpy()\n",
        "    sg = np.array(transforms.Resize((128, 256))(Image.fromarray(sg)))\n",
        "    spectrograms_1.append(np.log(sg + 1e-9))\n",
        "\n",
        "\n",
        "    sg = audio.MelSpectrogram( \n",
        "                            sample_rate = 44000, n_fft=4096, \n",
        "                            win_length = 2048, hop_length=512,\n",
        "                            n_mels=128)(sig)\n",
        "    sg = sg.detach().numpy()\n",
        "    sg = np.array(transforms.Resize((128, 256))(Image.fromarray(sg)))\n",
        "    spectrograms_2.append(np.log(sg + 1e-9))\n",
        "\n",
        "\n",
        "    sg = audio.MelSpectrogram( \n",
        "                            sample_rate = 44000, n_fft=4096, \n",
        "                            win_length = 1024, hop_length=256,\n",
        "                            n_mels=128)(sig)\n",
        "    sg = sg.detach().numpy()\n",
        "    sg = np.array(transforms.Resize((128, 256))(Image.fromarray(sg)))\n",
        "    spectrograms_3.append(np.log(sg + 1e-9))\n",
        "\n",
        "\n",
        "features_log = np.stack([[spectrograms_1[i], spectrograms_2[i], spectrograms_3[i]] for i in range(r)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t93rB0XE588d"
      },
      "source": [
        "# Sphering the spectrograms:\n",
        "ZCA_spectrograms_1 = []\n",
        "ZCA_spectrograms_2 = []\n",
        "ZCA_spectrograms_3 = []\n",
        "\n",
        "for i in range(r):\n",
        "    for j in range(3):\n",
        "        demeaned = np.array(features_log[i][j] - np.mean(features_log[i][j], axis=0))\n",
        "        (r_prime,c) = demeaned.shape\n",
        "        lam = 10**-7\n",
        "        P = float(c)\n",
        "        cov = 1/P*np.dot(demeaned, demeaned.T) + lam*np.eye(r_prime)\n",
        "        D,V = np.linalg.eigh(cov)\n",
        "        ZCAed = np.dot(V, np.dot(V.T, demeaned))\n",
        "        if j == 0:\n",
        "            ZCA_spectrograms_1.append(ZCAed)\n",
        "        if j == 1:\n",
        "            ZCA_spectrograms_2.append(ZCAed)\n",
        "        if j == 2:\n",
        "            ZCA_spectrograms_3.append(ZCAed)\n",
        "\n",
        "ZCA_features_log = np.stack([[ZCA_spectrograms_1[i], ZCA_spectrograms_2[i], ZCA_spectrograms_3[i]] for i in range(r)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gikYR-kA588e"
      },
      "source": [
        "np.save('extra_ZCA_features_1.npy', ZCA_features_log)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}